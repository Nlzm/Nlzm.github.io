<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文阅读——CLIP算法</title>
    <link href="/2023/09/18/clip/"/>
    <url>/2023/09/18/clip/</url>
    
    <content type="html"><![CDATA[<h1 id="论文阅读clip算法">论文阅读——CLIP算法</h1><p><strong>原文链接：</strong>[<ahref="https://arxiv.org/abs/2103.00020">2103.00020] LearningTransferable Visual Models From Natural Language Supervision(arxiv.org)</a></p><p><strong>代码链接：</strong><ahref="https://github.com/OpenAI/CLIP">openai/CLIP: CLIP (ContrastiveLanguage-Image Pretraining), Predict the most relevant text snippetgiven an image (github.com)</a></p><h2 id="算法原理">1、算法原理</h2><p>CLIP（Contrastive Language-ImagePre-training）具备很强的迁移学习能力。在无任意一张ImageNet图片训练情景下，直接进行Zero-shot推理，就能媲美监督训练下的ResNet-50模型的结果。</p><p>宏观来看CLIP分为三部分：</p><ul><li><p>Contrastive pre-training：预训练阶段，使用图片 -文本对进行对比学习训练；</p></li><li><p>Create dataset classifier from labeltext：提取预测类别文本特征;</p></li><li><p>Use for zero-shot predictiion：进行 Zero-Shoot 推理预测;</p></li></ul><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202309190948459.png"alt="image-20230917172037149" /><figcaption aria-hidden="true">image-20230917172037149</figcaption></figure><p>第一阶段，图像和文本分别通过图像、文本编码器生成对应的<spanclass="math inline">\(l_1、l_2…l_n\)</span>、<spanclass="math inline">\(T_1、T_2…T_n\)</span>的特征向量，计算对应角标向量的余弦相似度，通过temperature参数缩放，并借助softmax归一化为概率分布。图像编码器选用两个架构，第一个采用的是ResNet-50的基础架构，使用ResNetD和Rect-2进行改进，将全局平均池化层替换为一个单层的注意力池化机制；第二个采用改进的ViT模型。文本编码器使用的是一个Transformer编码器，有8个注意力头，使用了隐藏的自注意。</p><p>第二阶段，使用提示模板，帮助指定文本是否是关于图像的内容。将输出的句子通过文本编码器进行特征提取，得到特征向量。</p><p>第三阶段，输入一张图片，经过图像编码器进行特征提取生成一个特征向量，与文本特征进行余弦相似度计算，最相似的即为预测结果。</p><h2 id="代码实现">2、代码实现</h2><p>伪代码如下：</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202309181104648.png"alt="image-20230918110417534" /><figcaption aria-hidden="true">image-20230918110417534</figcaption></figure><p>CLIP模型前向传播部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, image, text</span>):<br>  image_features = self.encode_image(image)    <span class="hljs-comment"># 图片编码提特征</span><br>  text_features = self.encode_text(text)       <span class="hljs-comment"># 文本编码提特征</span><br>​<br>  <span class="hljs-comment"># 特征归一化</span><br>  image_features = image_features / image_features.norm(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>  text_features = text_features / text_features.norm(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>​<br>  <span class="hljs-comment"># 计算余弦相似度</span><br>  logit_scale = self.logit_scale.exp()<br>  logits_per_image = logit_scale * image_features @ text_features.t()<br>  logits_per_text = logits_per_image.t()<br>​<br>  <span class="hljs-comment"># shape = [global_batch_size, global_batch_size]</span><br>  <span class="hljs-keyword">return</span> logits_per_image, logits_per_text<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Multimodal Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Zero-shot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python数据处理——去除 NaN 值</title>
    <link href="/2023/09/18/python/"/>
    <url>/2023/09/18/python/</url>
    
    <content type="html"><![CDATA[<h1 id="python数据处理去除-nan-值">Python数据处理——去除 NaN 值</h1><h2 id="nan-的定义">1、NaN 的定义</h2><p>NaN 即 Not A Number 的缩写，表示不是一个数字。NaN值是在进行数学计算时出现的一种特殊值，通常出现在计算过程中出现了错误或无法计算的情况下。</p><h2 id="处理方法">2、处理方法</h2><ul><li>使用 dropna() 方法删除 NaN 值所在的行或列</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 读取Excel文件</span><br>file_path = <span class="hljs-string">&#x27;singleCoal.xlsx&#x27;</span><br>df = pd.read_excel(file_path)<br><br>df.dropna()  <span class="hljs-comment"># 删除包含 NaN 值的行</span><br><br>df.dropna(axis=<span class="hljs-string">&#x27;columns&#x27;</span>)  <span class="hljs-comment"># 删除包含 NaN 值的列，需要加上 axis 参数</span><br></code></pre></td></tr></table></figure><ul><li>使用 fillna() 方法填充 NaN 值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">df.fillna(value=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 将 NaN 值填充为 0</span><br><br><span class="hljs-comment">#使用 method 参数指定使用哪种方法进行填充</span><br>df.fillna(method=<span class="hljs-string">&quot;ffill&quot;</span>)  <span class="hljs-comment"># 前向填充</span><br>df.fillna(method=<span class="hljs-string">&quot;bfill&quot;</span>)  <span class="hljs-comment"># 后向填充</span><br></code></pre></td></tr></table></figure><ul><li>使用 interpolate() 方法插值填充 NaN 值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">df.interpolate()  <span class="hljs-comment"># 使用默认方式进行插值填充</span><br><br>df.interpolate(method=<span class="hljs-string">&quot;linear&quot;</span>)  <span class="hljs-comment"># 线性插值</span><br>df.interpolate(method=<span class="hljs-string">&quot;polynomial&quot;</span>, order=<span class="hljs-number">2</span>)  <span class="hljs-comment"># 二次插值</span><br>df.interpolate(method=<span class="hljs-string">&quot;spline&quot;</span>, order=<span class="hljs-number">2</span>)  <span class="hljs-comment"># 样条插值</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>data analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VScode服务器小问题</title>
    <link href="/2023/09/12/VScode%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B0%8F%E9%97%AE%E9%A2%98/"/>
    <url>/2023/09/12/VScode%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B0%8F%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="vscode服务器小问题">VScode服务器小问题</h1><h2id="服务器不断要求输入密码连不上服务器">1、服务器不断要求输入密码，连不上服务器</h2><p><strong>原因：</strong>没有恰当的退出远程链接导致的</p><p><strong>解决方法：</strong>打开VScode的 view→palette然后输入Kill VSCode Setver on Host，再重新连接就可以了。</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202309112035681.png"alt="image-20230911203505606" /><figcaption aria-hidden="true">image-20230911203505606</figcaption></figure><p>后续使用完远程服务器关闭资源的时候要使用File→Close RemoteConnection来关闭资源。</p>]]></content>
    
    
    <categories>
      
      <category>Server</category>
      
    </categories>
    
    
    <tags>
      
      <tag>服务器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学习笔记——XGBoost算法</title>
    <link href="/2023/09/11/XGBoost%E7%AE%97%E6%B3%95/"/>
    <url>/2023/09/11/XGBoost%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="学习笔记xgboost算法">学习笔记——XGBoost算法</h1><p>XGBoost和GBDT两者都是boosting方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。</p><h2 id="基本原理">1、基本原理</h2><h3 id="目标函数">1.1 目标函数</h3><p>XGBoost算法<strong>是一个加法模型</strong>，在每一步迭代中，只调优当前的子模型：<spanclass="math inline">\(F_m(x_i)=F_{m-1}(x_i)+f_m(x_i)\)</span>。其中<spanclass="math inline">\(f_m(x_i)\)</span>表示当前的子模型，<spanclass="math inline">\(F_{m-1}(x_i)\)</span>表示前<spanclass="math inline">\(m-1\)</span>个已固定模型。</p><p>目标函数=经验风险+结构风险（正则项）： <span class="math display">\[\begin{aligned}obj&amp; =\sum_{i=1}^NL[F_m(x_i),y_i]+\sum_{j=1}^m\Omega(f_j)  \\&amp;=\sum_{i=1}^NL[F_{m-1}(x_i)+f_m(x_i),y_i]+\sum_{j=1}^m\Omega(f_j)\end{aligned}\]</span></p><p>其中正则项<spanclass="math inline">\(\Omega(f_j)\)</span>表示模型<spanclass="math inline">\(f\)</span>的复杂度。</p><p>XGBoost用2阶泰勒公式：<span class="math inline">\(f(x_0+\Deltax)\approx f(x_0)+f^{&#39;}(x_0)\Deltax+\frac{f^{&#39;&#39;}(x_0)}{2}(\Delta x)^2\)</span>​来逼近损失函数，我们可以将<spanclass="math inline">\(F_{m-1}(x_i)\)</span>看作<spanclass="math inline">\(x_0\)</span>,将<spanclass="math inline">\(f_m(x_i)\)</span>看作<spanclass="math inline">\(\Delta x\)</span>,所以（1）式就可以转化为： <spanclass="math display">\[\begin{aligned}Obj=\sum_{i=1}^N\left[L[F_{m-1}(x_i),y_i]+\frac{\partialL}{\partialF_{m-1}(x_i)}f_m(x_i)+\frac{1}{2}\frac{\partial^2L}{\partial^2F_{m-1}(x_i)}f_m^2(x_i)\right]+\\\sum_{j=1}^m\Omega(f_j)\end{aligned}\]</span> 由于前m-1个模型是确定的，所以<spanclass="math inline">\(\sum_{j=1}^m\Omega(f_j)\)</span>，前m-1项均为常数，对目标函数的求解无影响，所以（2）式又可以转化为：<span class="math display">\[Obj=\sum_{i=1}^{N}\left[g_{i}f_{m}(x_{i})+\frac{1}{2}h_{i}f_{m}^{2}(x_{i})\right]+\Omega(f_{m})\]</span></p><h3 id="基于树的正则化">1.2 基于树的正则化</h3><p>XGBoost支持<strong>的基分类器包括决策树和线性模型</strong>，为<strong>防止过拟合</strong>，XGBoost将<strong>树的深度设置为正则项</strong>：<spanclass="math inline">\(\Omega(f)=\gammaT+\frac{1}{2}\lambda||w||^{2}\)</span>，其中<spanclass="math inline">\(\gamma\)</span>和<spanclass="math inline">\(\lambda\)</span>作为超参数。所以目标函数可以改写为：<span class="math display">\[Obj=\sum_{i=1}^{N}\left[g_if_m(x_i)+\frac{1}{2}h_if_m^2(x_i)\right]+\gammaT+\frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2\]</span>通过数学处理，可以将正则项和经验风险项合并：将经验风险项从样本层面上求和转换为叶节点层面上的求和。可以定义结点j上的样本集为<spanclass="math inline">\(I(j)=\{x_{i}|q(x_{i})=j\}\)</span>,其中<spanclass="math inline">\(q(x_i)\)</span>为将样本映射到叶节点上的索引函数，叶节点<spanclass="math inline">\(j\)</span>上的回归值为<spanclass="math inline">\(w_{j}=f_{m}(x_{i}),i\in I(j)\)</span>.</p><p>所以式（4）进一步简化,令<span class="math inline">\(\sum_{i\inI(j)}g_i=G_j,\sum_{i\in I(j)}h_i=H_j\)</span>： <spanclass="math display">\[Obj=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gammaT\]</span> 如果一棵树的结构是确定的，则各个节点内的样本（<spanclass="math inline">\(x_i,y_i,g_i,h_i,G_j,H_j,T\)</span>）也是确定的，每个叶子结点输出回归值应该使得式（5）最小，所以该函数的二次函数极值点为：<spanclass="math inline">\(w_j^*=-\frac{G_j}{H_j+\lambda}\)</span></p><p>树的评分也可以理解成所有叶节点的评分之和：<spanclass="math inline">\(Obj^*=\sum_{j=1}^T\left(-\frac{1}{2}\frac{G_j^2}{H_j+\lambda}+\gamma\right)\)</span>.</p><h3 id="结点分裂准则">1.3 结点分裂准则</h3><p>XGBoost的子模型树和决策树模型一样，要依赖<strong>节点递归分裂的贪心准则来实现树的生成</strong>：</p><p>从树的深度为0开始：</p><ol type="1"><li>对每个叶节点枚举所有的可用特征；</li><li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li><li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集；</li><li>回到第1步，递归执行直到满足特定条件为止；</li></ol><p>过程如图所示：</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202309111523571.png"alt="image-20230911152316481" /><figcaption aria-hidden="true">image-20230911152316481</figcaption></figure><p>显然分裂收益是树A的评分减去树B的评分，因此分裂收益表达式为： <spanclass="math display">\[Gain=\frac12\Big[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\Big]-\gamma\]</span><strong>XGBoost还支持近似算法</strong>：根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。具体而言，特征分位数的选取有global和local两种可选策略：</p><ul><li>global在全体样本上的特征值中选取，在根节点分裂之前进行一次即可；</li><li>local则是在待分裂节点包含的样本特征值上选取，每个节点分裂前都要进行。</li></ul><p>通常，global由于只能划分一次，其划分粒度需要更细。通过这个方法可以解决数据量过大超过内存、或有并行计算需求的情况。</p><p>由（3）式可得：令其偏导为0可以得到<spanclass="math inline">\(f_{m}^{*}(x_{i})=-\frac{g_{i}}{h_{i}}\)</span>,此目标函数可理解为以<spanclass="math inline">\(h_i\)</span>为权重，<spanclass="math inline">\(-\frac{g_{i}}{h_{i}}\)</span>​为标签的二次损失函数：<span class="math display">\[\begin{aligned}obj&amp;=\sum_{i=1}^{N}\left[g_{i}f_{m}(x_{i})+\frac{1}{2}h_{i}f_{m}^{2}(x_{i})\right]+\Omega(f_{m})  \\&amp;=\sum_{i=1}^{N}\frac{1}{2}h_{i}\Big[f_{m}(x_{i})-(-\frac{g_{i}}{h_{i}})\Big]^{2}+\Omega(f_{m})+C\end{aligned}\]</span> 因此，在近似算法取分位数时，实际上XGBoost会取以二阶导<spanclass="math inline">\(h_i\)</span>为权重的分位数。</p><h3 id="列采样和学习率">1.4 列采样和学习率</h3><p><strong>XGBoost还引入了两项特性：列采样和学习率</strong></p><ul><li>列采样：即随机森林中的做法，每次节点分裂的待选特征集合不是剩下的全部特征，而是剩下特征的一个子集。是为了更好地对抗过拟合，还能减少计算开销。</li><li>学习率：或者叫步长、shrinkage，是在每个子模型前（即在每个叶节点的回归值上）乘上该系数，削弱每颗树的影响，使得迭代更稳定。可以类比梯度下降中的学习率。XGBoost默认设定为0.3。</li></ul><h3 id="稀疏感知">1.5 稀疏感知</h3><p>XGBoost将缺失值和稀疏0值等同视作缺失值，将这些缺失值“绑定”在一起，分裂结点的遍历则会跳过缺失值整体，提升了运算效率。</p><h2 id="工程优化">2. 工程优化</h2><h3 id="并行列块的设计">2.1 并行列块的设计</h3><p>XGBoost将每一列特征提前进行排序，以块（Block）的形式储存在缓存中，并以索引将特征值和梯度统计量<spanclass="math inline">\(g_i,h_i\)</span>对应起来，每次节点分裂时会重复调用排好序的块。而且不同特征会分布在独立的块中，因此可以进行分布式或多线程的计算。</p><h3 id="缓存访问">2.2 缓存访问</h3><p>特征值排序后通过索引来取梯度 <spanclass="math inline">\(g_i,h_i\)</span>会导致访问的内存空间不一致，进而降低缓存的命中率，影响算法效率。为解决这个问题，XGBoost为每个线程分配一个单独的连续缓存区，用来存放梯度信息。</p><h3 id="核外块计算">2.3 核外块计算</h3><p>数据量过大时，不能同时全部载入内存。XGBoost将数据分为多个blocks并储存在硬盘中，使用一个独立的线程专门从磁盘中读取数据到内存中，实现计算和读取数据的同时进行。为了进一步提高磁盘读取数据性能，XGBoost还使用了两种方法：</p><ul><li><strong>块压缩：</strong>通过压缩block，用解压缩的开销换取磁盘读取的开销；</li><li><strong>块分区：</strong>将block分散储存在多个磁盘中，有助于提高磁盘吞吐量。</li></ul><h2 id="与gbdt比较">3. 与GBDT比较</h2><ul><li>性质：GBDT是机器学习算法，XGBoost除了算法内容还包括一些工程实现方面的优化。</li><li>基于二阶导：GBDT使用的是损失函数一阶导数，相当于函数空间中的梯度下降；而XGBoost还使用了损失函数二阶导数，相当于函数空间中的牛顿法。</li><li>正则化：XGBoost显式地加入了正则项来控制模型的复杂度，能有效防止过拟合。</li><li>列采样：XGBoost采用了随机森林中的做法，每次节点分裂前进行列随机采样。</li><li>缺失值处理：XGBoost运用稀疏感知策略处理缺失值，而GBDT没有设计缺失策略。</li><li>并行高效：XGBoost的列块设计能有效支持并行运算，提高效率。</li></ul><h4 id="缺点">缺点</h4><ul><li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li><li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li></ul><h2 id="参考资料">参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/142413825?utm_id=0">机器学习 |XGBoost详解 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/83901304">深入理解XGBoost -知乎 (zhihu.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>boosting</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学习笔记——LORA微调</title>
    <link href="/2023/08/16/LORA%E5%BE%AE%E8%B0%83/"/>
    <url>/2023/08/16/LORA%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="学习笔记lora微调">学习笔记——LORA微调</h1><p>​ LORA是一种低资源微调LLM模型的方法，源自论文：<ahref="%5B2106.09685.pdf%20(arxiv.org)%5D(https://arxiv.org/pdf/2106.09685.pdf)">LoRA:Low-Rank Adaptation of Large Language Models。</a></p><h2 id="一高效微调">一、高效微调</h2><p>​ 对于语言模型来说，在微调过程中，模型加载预训练参数<spanclass="math inline">\(\Phi_{0}\)</span>进行初始化，并通过最大化条件语言模型概率实现参数调整$_{0}+$​，即：<span class="math display">\[max_\Phi\sum_{(x,y)\in\mathcal{Z})}\sum_{t=1}^{|y|}log(P_\Phi(y_t|x,y&lt;t))\]</span> 这种方式的主要缺点参数增量<spanclass="math inline">\(\Delta\Phi\)</span>的维度和预训练参数<spanclass="math inline">\(\Phi_{0}\)</span>是相同的，所需资源比较多，一般被称为fullfine-tuing。</p><p>为了用更少的参数来表示学习增量<spanclass="math inline">\(\Delta\Phi\)</span>,提出了一系列方法叫做高效微调。例如：Adapter、prefixtuning等。相比于其他方法LORA使用一个低秩矩阵来编码参数增量，这种方法不会增加推理耗时并且便于优化。</p><h2 id="二实现方式">二、实现方式</h2><p>​ 研究表明：预训练模型拥有极小的内在维度（<strong>instrisicdimension</strong>，表示数据变化的自由变量的个数），换而言之，存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果。</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202308161517289.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>该论文认为参数矩阵更新的过程中也存在一个‘内在秩’。对于预训练的权重矩阵<spanclass="math inline">\(W_0\)</span>，可以用一个低秩分解来表示参数更新<spanclass="math inline">\(\Delta W\)</span>，即： <spanclass="math display">\[W_0+\Delta W=W_0+BA\quad B\in\mathbb{R}^{d\timesr},A\in\mathbb{R}^{r\times k}\quad and\quad r\ll min(d,k)\]</span> 训练过程中可以冻结参数<spanclass="math inline">\(W_0\)</span>，只训练A和B中的参数。如图所示，对于<spanclass="math inline">\(h=W_{0}x\)</span>​,前向传播过程就变为： <spanclass="math display">\[h=W_0x+\Delta Wx=W_0x+BAx\]</span></p><h2 id="三qlora">三、<ahref="%5B2305.14314.pdf%20(arxiv.org)%5D(https://arxiv.org/pdf/2305.14314.pdf)">QLORA</a></h2><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202308161959472.png"alt="image-20230816195941425" /><figcaption aria-hidden="true">image-20230816195941425</figcaption></figure><p>QLORA训练过程和LORA基本一致，区别在于QLORA模型是按照NF4保存的，训练时需要将参数反量化到bf16后进行训练。<span class="math display">\[Y^{BF16}=X^{BF16}doubleDequant(c_{2}^{FP32},c_{2}^{k-bit},W^{NF8})+X^{BF16}B^{BF16}A^{BF16}\]</span></p><h3 id="分块量化block-wis-quantization">分块量化（Block-wisQuantization）</h3><p>量化是将输入从存储更多信息的表征映射为存储较少信息的表征的过程。</p><p>全局量化的方式存在一个问题：当输入中存在极大值或者离群值时，一些较小的参数无法被精确的表示，从而导致量化后的神经网络效果急剧下降。</p><p>分块量化是将输入划分为多个块，每个块分别量化，如图所示：</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202308161954660.png"alt="image-20230816195439589" /><figcaption aria-hidden="true">image-20230816195439589</figcaption></figure><p>明显看到分块量化能够减少过程中的误差。</p><h3 id="分位量化quantile-quantization">分位量化（QuantileQuantization）</h3><p>在将一个参数量化到4bit的情境中，最多可以使用<spanclass="math inline">\(2^4\)</span>一共16个数字。按照传统的简易方法，一般是取最接近的数字或者直接round函数。分位量化则是将数字按顺序排列，再分为十六等分，最小的一块映射成量化后的第一个数，第二块映射成量化后的第二数，以此类推。这样就充分利用了已有的数位，原始数据在量化后的数字上分布也是均匀的。</p><h3 id="bit-normalfloatnf4">4-bit NormalFloat（NF4）</h3><p>这个概念是在分位量化的基础上进行改进，并结合分块量化，降低计算复杂度和误差。上述的分位量化会增加计算消耗，预训练模型的参数基本上都服从均值为0的正态分布，可以将其缩放到[-1,1]的范围内。同时可以在[-1,1]的范围内，将正态分布函数划分为<spanclass="math inline">\(2^k+1\)</span>份，直接将参数映射到对应的分位上，不用每次都进行排序。</p><h3 id="双重量化double-quantization">双重量化（doubleQuantization）</h3><p>分块量化中每个块都会额外产生一个量化常数c，以块大小为64为例，每个块会产生32bit的量化常数，双重量化则是在第一次量化后，不会直接存储量化常数<spanclass="math inline">\(C_1\)</span>,而是按照块大小256对量化常数再量化为8bit去存储，这个阶段会产生一个量化常数<spanclass="math inline">\(C_2\)</span>。最终存储的参数为<spanclass="math inline">\(8/64 +32/(64-256)=0.127bits\)</span>.</p><h2 id="四adalora">四、AdaLORA</h2><h3 id="技术背景">技术背景</h3><p>LORA技术预先规定每个增量矩阵<spanclass="math inline">\(\Delta\)</span>的秩必须相同，这就忽略了不同层、类型参数对下游任务的影响。</p><figure><imgsrc="https://2hang.oss-cn-beijing.aliyuncs.com/img/202308162040469.png"alt="image-20230816203952264" /><figcaption aria-hidden="true">image-20230816203952264</figcaption></figure><p>如图所示，将微调参数放在FFN的效果优于放在Attention矩阵中的效果；同时微调高层参数的效果优于微调底层参数。那么如何根据下游任务自动地找出重要的参数模块并给其分配更多地可微调参数呢？</p><h3 id="解决方案">解决方案</h3><p>AdaLORA主要包含两个模块：</p><ul><li><p><strong>SVD形式参数更新（SVD-basedadaptation）</strong>：直接将增量矩阵<spanclass="math inline">\(\Delta\)</span>参数化为SVD地形式，避免在训练过程中进行SVD计算带来的计算资源消耗；</p></li><li><p><strong>根据重要程度地参数分配（Importance-aware rankallocation）</strong>：去除一些冗余的奇异值。</p></li></ul><p><span class="math display">\[W=W^{(0)}+\Delta=W^{(0)}+P\Lambda Q\]</span></p><p><span class="math display">\[R(P,Q)=||P^TP-I||_F^2+||Q^TQ-I||_F^2\]</span></p><p>如式(5)，AdaLORA增量矩阵<spanclass="math inline">\(\Delta\)</span>替换为<spanclass="math inline">\(P\LambdaQ\)</span>,这样既省去复杂的SVD计算又能去除奇异值。同时，为保证P和Q的正交性，在训练过程中增加了一个正则化，保证<spanclass="math inline">\(P^{T}P=Q^{T}Q=I\)</span>.</p><p>该方法相较于LORA有两个优点：</p><ul><li>AdaLORA只去除奇异值矩阵，并不会去除奇异向量，更容易恢复误删的奇异值。</li><li>AdaLORA的P和Q为正交举证，LORA的A和B矩阵非正交。训练过程中裁剪操作不会影响其他奇异值对应的奇异向量，训练会更稳定，泛化性能更好。</li></ul><h2 id="参考文章">参考文章：</h2><p><ahref="https://zhuanlan.zhihu.com/p/646791309">LORA微调系列(一)：LORA和它的基本原理- 知乎 (zhihu.com)</a></p><p><ahref="https://zhuanlan.zhihu.com/p/648239462">LORA微调系列(二)：QLORA和它的基本原理- 知乎 (zhihu.com)</a></p><p><ahref="https://zhuanlan.zhihu.com/p/649756885">LORA微调系列(三)：AdaLORA和它的基本原理- 知乎 (zhihu.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>微调</tag>
      
      <tag>实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
